{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import os\n",
    "from torch.autograd import grad\n",
    "from torch import nn\n",
    "import random\n",
    "os.chdir(os.path.split(os.getcwd())[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "torch.Size([1000, 3])\n",
      "torch.Size([1000, 1000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaoid\\AppData\\Local\\Temp\\ipykernel_13804\\665812767.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  solutionValue = torch.tensor(f[\"solutionValues\"]).T\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\".\\\\dataset\\\\unitData.h5\", 'r') as f:\n",
    "  d = list(f[\"solEvalPoints\"])\n",
    "  # IDs of nodes used as trunk network inputs\n",
    "  nodeID = list(map(int, d[0]))\n",
    "  # [x y t] of y points (trunk inputs).\n",
    "  # matrix with rows [x y t]\n",
    "  solEvalPoint = torch.tensor(np.concatenate([d[1][:, None], d[2][:, None], d[3][:, None]], axis = 1))\n",
    "  # DeepONet output targets (G(u)(y)) for all initial conditions\n",
    "  # matrix with 'initial condition' rows and 'y point' columns\n",
    "  solutionValue = torch.tensor(f[\"solutionValues\"]).T\n",
    "del d\n",
    "print(len(nodeID))\n",
    "print(solEvalPoint.shape)\n",
    "print(solutionValue.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 1000000 samples.\n"
     ]
    }
   ],
   "source": [
    "# number of sensors is equal to number of cases\n",
    "nCase = solutionValue.shape[0]\n",
    "nSensor = solutionValue.shape[0]\n",
    "# value of concentrated load as initial condition\n",
    "load = torch.tensor(-1.21 * 0.02).to(torch.float32)\n",
    "evalAmount = solEvalPoint.shape[0] # number of y points (trunk inputs)\n",
    "nSample = torch.numel(solutionValue) # amount of samples in dataset\n",
    "print(f\"Dataset contains {nSample} samples.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class acoustDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset subclass to organize data\"\"\"\n",
    "    def __init__(self, evalPoint: torch.tensor, solVal: torch.tensor) -> None:\n",
    "        super(acoustDataset, self).__init__()\n",
    "        self.trunkInput = evalPoint\n",
    "        # target for model output\n",
    "        self.target = torch.flatten(solVal)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Overwrite method to fetch sample\"\"\"\n",
    "        # branch input load value at specific sensor. 0 everywhere else\n",
    "        branchInput = torch.zeros(nSensor, dtype = torch.float32)\n",
    "        branchInput[idx - (idx // nCase) * nCase] = load\n",
    "        branchInput.requires_grad_(True)\n",
    "        # index to access tensor with coordinates of evaluation point (trunk inputs)\n",
    "        i = idx - (idx // evalAmount) * evalAmount\n",
    "        trunkIn = [self.trunkInput[i, s].unsqueeze(0).requires_grad_(True) for s in range(3)]\n",
    "        # sample has (case evaluation at sensors, x, y, t, G(u)(y))\n",
    "        return branchInput, *trunkIn, self.target[idx].unsqueeze(0).requires_grad_(True)\n",
    "        # return (branchInput, self.trunkInput[i, 0].unsqueeze(0), self.trunkInput[i, 1].unsqueeze(0), self.trunkInput[i, 2].unsqueeze(0), self.target[idx].unsqueeze(0))\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Overwrite method to return size of dataset\"\"\"\n",
    "        return torch.numel(self.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes:\n",
      "  branch network inputs: torch.Size([64, 1000])\n",
      "  trunk network inputs:\n",
      "    x torch.Size([64, 1])\n",
      "    y torch.Size([64, 1])\n",
      "    t torch.Size([64, 1])\n",
      "  model output targets: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(acoustDataset(solEvalPoint, solutionValue),\n",
    "    batch_size = 64, shuffle = True, num_workers = 0\n",
    ")\n",
    "branchBatch, xBatch, yBatch, tBatch, targetBatch = next(iter(dataloader))\n",
    "print(f\"\"\"Batch shapes:\n",
    "  branch network inputs: {branchBatch.shape}\n",
    "  trunk network inputs:\n",
    "    x {xBatch.shape}\n",
    "    y {yBatch.shape}\n",
    "    t {tBatch.shape}\n",
    "  model output targets: {targetBatch.shape}\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "branchDepth = 7 # number of layers in branch network (MLP architecture)\n",
    "branchWidth = 100 # number of neurons in layers of branch network (MLP architecture)\n",
    "trunkDepth = 7 # number of layers in trunk network (MLP architecture)\n",
    "trunkWidth = 100 # number of neurons in layers of trunk network (MLP architecture)\n",
    "networkOutputDim = 100 # dimension of outputs of networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Branch - batched input torch.Size([64, 1000]) -> output: torch.Size([64, 100]).\n",
      "Trunk - batched input (torch.Size([64, 1]), torch.Size([64, 1]), torch.Size([64, 1])) -> output: torch.Size([64, 100]).\n",
      "Batch PI-DeepONet output shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "class branchNet(nn.Module):\n",
    "    \"\"\"Branch network definition\"\"\"\n",
    "    def __init__(self, inDim: int, nnDepth: int, nnWidth: int):\n",
    "        super().__init__()\n",
    "        # Input layer. Resizes input to desired network width\n",
    "        self.inputLayer = nn.Linear(inDim, nnWidth)\n",
    "        # intermediate dense layers. constant dimension\n",
    "        self.MLPstack = nn.ModuleList([nn.Linear(nnWidth, nnWidth) for _ in range(nnDepth - 2)])\n",
    "        # output layer. resizes network intermediate representation to networkOutputDim\n",
    "        self.outputLayer = nn.Linear(nnWidth, networkOutputDim)\n",
    "\n",
    "    def forward(self, x): # forward pass\n",
    "        x = self.inputLayer(x)\n",
    "        for l in self.MLPstack:\n",
    "            x = l(x)\n",
    "        return self.outputLayer(x)\n",
    "    \n",
    "class trunkNet(nn.Module):\n",
    "    \"\"\"Trunk network definition\"\"\"\n",
    "    def __init__(self, nnDepth: int, nnWidth: int):\n",
    "        super().__init__()\n",
    "        # Input layer. Resizes input to desired network width.\n",
    "        # consider trunk network as receiving individual inputs\n",
    "        # for each dimension (x, y, t)\n",
    "        self.xCoord = nn.Linear(1, nnWidth)\n",
    "        self.yCoord = nn.Linear(1, nnWidth)\n",
    "        self.tCoord = nn.Linear(1, nnWidth)\n",
    "        # intermediate dense layers. constant dimension\n",
    "        self.MLPstack = nn.ModuleList([nn.Linear(nnWidth, nnWidth) for _ in range(nnDepth - 2)])\n",
    "        # output layer. resizes network intermediate representation to networkOutputDim\n",
    "        self.outputLayer = nn.Linear(nnWidth, networkOutputDim)\n",
    "\n",
    "    def forward(self, x, y, t): # forward pass\n",
    "        x = self.xCoord(x)\n",
    "        y = self.yCoord(y)\n",
    "        t = self.tCoord(t)\n",
    "        o = x + y + t\n",
    "        for l in self.MLPstack:\n",
    "            o = l(o)\n",
    "        return self.outputLayer(o)\n",
    "    \n",
    "# Branch network. Embeds evaluations of the input functions 'u' at sensors\n",
    "branchNetwork = branchNet(nSensor, branchDepth, branchWidth)\n",
    "branchOut = branchNetwork(branchBatch)\n",
    "# Trunk network. Embeds spatiotemporal coordinates (x, y, t) of\n",
    "# point y, in which the PDE solution (G(u)(y)) is evaluated\n",
    "trunkNetwork = trunkNet(trunkDepth, trunkWidth)\n",
    "trunkOut = trunkNetwork(xBatch, yBatch, tBatch)\n",
    "# Shapes of outputs of networks\n",
    "print(f\"\"\"Branch - batched input {branchBatch.shape} -> output: {branchOut.shape}.\n",
    "Trunk - batched input ({xBatch.shape}, {yBatch.shape}, {tBatch.shape}) -> output: {trunkOut.shape}.\"\"\")\n",
    "\n",
    "class PI_deepONet(nn.Module):\n",
    "    \"\"\"Class for physics-informed DeepONet\"\"\"\n",
    "    def __init__(self, branch: nn.Module, trunk: nn.Module):\n",
    "        super().__init__()\n",
    "        self.branch = branch\n",
    "        self.trunk = trunk\n",
    "\n",
    "    def forward(self, case: torch.tensor, x: torch.tensor, y: torch.tensor, t: torch.tensor):\n",
    "        # transpose trunk output and multiply both for\n",
    "        # sample-by-sample dot product in diagonal of result\n",
    "        return torch.diagonal(torch.matmul(self.branch(case), self.trunk(x, y, t).T))\n",
    "    \n",
    "model = PI_deepONet(branchNetwork, trunkNetwork)\n",
    "modelOut = model(branchBatch, xBatch, yBatch, tBatch)\n",
    "print(f\"Batch PI-DeepONet output shape: {modelOut.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (660270611.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 14\u001b[1;36m\u001b[0m\n\u001b[1;33m    p_x = torch.autograd.grad(p, , create_graph=True)[0]\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# outM = model(branchBatch, xBatch, yBatch, tBatch)\n",
    "# p_x = grad(outM.sum(), xBatch, create_graph = True)[0]\n",
    "# p_xx = grad(p_x.sum(), xBatch)[0]\n",
    "# print(p_x.shape)\n",
    "# print(p_xx.shape)\n",
    "\n",
    "# def sf(bb, xb, yb, tb):\n",
    "#     return torch.sum(model(bb, xb, yb, tb))\n",
    "# torch.autograd.functional.hessian(sf, (branchBatch, xBatch, yBatch, tBatch), create_graph = False, strict = False)\n",
    "\n",
    "def PDEresidual(case: torch.tensor, evalPoint: torch.tensor, m: torch.Module) -> torch.tensor:\n",
    "    \"\"\"PDE residual through AD for physics-informed loss term\"\"\"\n",
    "    p = model(case, evalPoint)\n",
    "    # p_x = torch.autograd.grad(p, , create_graph=True)[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH   0\n",
      "\n",
      "loss: 0.5413976311683655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaoid\\AppData\\Local\\Temp\\ipykernel_13804\\1347613132.py:11: UserWarning: Using a target size (torch.Size([64, 1])) that is different to the input size (torch.Size([64])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_fn(modelOut, sampleG_uy)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.005389668978750706\n",
      "loss: 0.012681028805673122\n",
      "loss: 0.008139814250171185\n",
      "loss: 0.00709192780777812\n",
      "loss: 0.006987951695919037\n",
      "loss: 0.013568645343184471\n",
      "loss: 0.017612842842936516\n",
      "loss: 0.006359223276376724\n",
      "loss: 0.022259794175624847\n",
      "loss: 0.016716646030545235\n",
      "loss: 0.01087149977684021\n",
      "loss: 0.007696544751524925\n",
      "loss: 0.00803461018949747\n",
      "loss: 0.02074422501027584\n",
      "loss: 0.00559166120365262\n",
      "loss: 0.00780774699524045\n",
      "loss: 0.004685216583311558\n",
      "loss: 0.009502287954092026\n",
      "loss: 0.017272019758820534\n",
      "loss: 0.009355535730719566\n",
      "loss: 0.009582115337252617\n",
      "loss: 0.005427428521215916\n",
      "loss: 0.01159230899065733\n",
      "loss: 0.012830229476094246\n",
      "loss: 0.004507889971137047\n",
      "loss: 0.011269123293459415\n",
      "loss: 0.017415864393115044\n",
      "loss: 0.008325275033712387\n",
      "loss: 0.010523764416575432\n",
      "loss: 0.013874617405235767\n",
      "loss: 0.01006183959543705\n",
      "loss: 0.006661584600806236\n",
      "loss: 0.008424133993685246\n",
      "loss: 0.005464915186166763\n",
      "loss: 0.016221430152654648\n",
      "loss: 0.013726095668971539\n",
      "loss: 0.017993604764342308\n",
      "loss: 0.006355004385113716\n",
      "loss: 0.008607851341366768\n",
      "loss: 0.005234905052930117\n",
      "loss: 0.011781524866819382\n",
      "loss: 0.006728901993483305\n",
      "loss: 0.012734680436551571\n",
      "loss: 0.008172429166734219\n",
      "loss: 0.009362826123833656\n",
      "loss: 0.020208174362778664\n",
      "loss: 0.006766776088625193\n",
      "loss: 0.015950972214341164\n",
      "loss: 0.0071908412501215935\n",
      "loss: 0.0056745256297290325\n",
      "loss: 0.006504203658550978\n",
      "loss: 0.009169758297502995\n",
      "loss: 0.009562497027218342\n",
      "loss: 0.006112250965088606\n",
      "loss: 0.008492580614984035\n",
      "loss: 0.00684509240090847\n",
      "loss: 0.014964796602725983\n",
      "loss: 0.005187868140637875\n",
      "loss: 0.004780986811965704\n",
      "loss: 0.0073533570393919945\n",
      "loss: 0.018520114943385124\n",
      "loss: 0.009952299296855927\n",
      "loss: 0.008405620232224464\n",
      "loss: 0.009827232919633389\n",
      "loss: 0.007061394862830639\n",
      "loss: 0.019592344760894775\n",
      "loss: 0.018019892275333405\n",
      "loss: 0.012989102862775326\n",
      "loss: 0.00658025499433279\n",
      "loss: 0.008542430587112904\n",
      "loss: 0.007748511619865894\n",
      "loss: 0.0065146200358867645\n",
      "loss: 0.007935645990073681\n",
      "loss: 0.006485176272690296\n",
      "loss: 0.005692458711564541\n",
      "loss: 0.00685265613719821\n",
      "loss: 0.013249782845377922\n",
      "loss: 0.0043633501045405865\n",
      "loss: 0.004831315018236637\n",
      "loss: 0.006356387864798307\n",
      "loss: 0.004143014084547758\n",
      "loss: 0.006947926711291075\n",
      "loss: 0.010432635433971882\n",
      "loss: 0.008552683517336845\n",
      "loss: 0.01827952265739441\n",
      "loss: 0.009641535580158234\n",
      "loss: 0.007292111404240131\n",
      "loss: 0.006700452417135239\n",
      "loss: 0.013006673194468021\n",
      "loss: 0.006076236721128225\n",
      "loss: 0.014252861961722374\n",
      "loss: 0.01800950989127159\n",
      "loss: 0.01585383713245392\n",
      "loss: 0.008283086121082306\n",
      "loss: 0.012465260922908783\n",
      "loss: 0.0064976890571415424\n",
      "loss: 0.005779976490885019\n",
      "loss: 0.00733464490622282\n",
      "loss: 0.017887625843286514\n",
      "loss: 0.005665729753673077\n",
      "loss: 0.01213366724550724\n",
      "loss: 0.011637053452432156\n",
      "loss: 0.02220962755382061\n",
      "loss: 0.0056149899028241634\n",
      "loss: 0.011663628742098808\n",
      "loss: 0.018116984516382217\n",
      "loss: 0.013522911816835403\n",
      "loss: 0.0048238495364785194\n",
      "loss: 0.00800430029630661\n",
      "loss: 0.01694738119840622\n",
      "loss: 0.005921130068600178\n",
      "loss: 0.010225534439086914\n",
      "loss: 0.02887492999434471\n",
      "loss: 0.0053938343189656734\n",
      "loss: 0.009392548352479935\n",
      "loss: 0.011726787313818932\n",
      "loss: 0.010548962280154228\n",
      "loss: 0.006623329594731331\n",
      "loss: 0.017073936760425568\n",
      "loss: 0.008318966254591942\n",
      "loss: 0.008372566662728786\n",
      "loss: 0.006751170381903648\n",
      "loss: 0.018849769607186317\n",
      "loss: 0.0053117708303034306\n",
      "loss: 0.008450046181678772\n",
      "loss: 0.015910709276795387\n",
      "loss: 0.008801202289760113\n",
      "loss: 0.005087796598672867\n",
      "loss: 0.011519836261868477\n",
      "loss: 0.010521331802010536\n",
      "loss: 0.006099135614931583\n",
      "loss: 0.009101868607103825\n",
      "loss: 0.007521037943661213\n",
      "loss: 0.015428836457431316\n",
      "loss: 0.02981683611869812\n",
      "loss: 0.01099465973675251\n",
      "loss: 0.009872755035758018\n",
      "loss: 0.009931126609444618\n",
      "loss: 0.006227393168956041\n",
      "loss: 0.008596801199018955\n",
      "loss: 0.006661189720034599\n",
      "loss: 0.006425580009818077\n",
      "loss: 0.010648815892636776\n",
      "loss: 0.010790407657623291\n",
      "loss: 0.008138039149343967\n",
      "loss: 0.00537673756480217\n",
      "loss: 0.01098488736897707\n",
      "loss: 0.00793872307986021\n",
      "loss: 0.007401606999337673\n",
      "loss: 0.014271009713411331\n",
      "loss: 0.007978570647537708\n",
      "loss: 0.012593410909175873\n",
      "loss: 0.007895193062722683\n",
      "loss: 0.012385650537908077\n",
      "loss: 0.005833963397890329\n",
      "loss: 0.006909092888236046\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, deepONet, loss_fn, optimizer, epochAmount):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode. Added for best practices\n",
    "    deepONet.train()\n",
    "    # iterate in batches\n",
    "    for epoch in range(epochAmount):\n",
    "        print(f\"EPOCH   {epoch}\\n\")\n",
    "        for (batch, (sampleCase, sampleX, sampleY, sampleT, sampleG_uy)) in enumerate(dataloader):\n",
    "            # Compute prediction and loss\n",
    "            modelOut = deepONet(sampleCase, sampleX, sampleY, sampleT)\n",
    "            loss = loss_fn(modelOut, sampleG_uy)\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                # loss, current = loss.item(), (batch + 1) * len(dataloader)\n",
    "                # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "                print(f\"loss: {loss.item()}\")\n",
    "\n",
    "train_loop(dataloader, model, torch.nn.functional.mse_loss, opt, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acousticPINNs-v9ZJkn9d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
