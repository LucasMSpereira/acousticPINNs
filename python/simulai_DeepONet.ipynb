{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 26,
            "id": "d2422554-cb16-49d8-88e3-6bc2bd8e4e37",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import h5py\n",
                "import matplotlib.pyplot as plt\n",
                "import random\n",
                "from collections import Counter\n",
                "import simulai\n",
                "from simulai.regression import DenseNetwork\n",
                "from simulai.models import DeepONet\n",
                "# from simulai.optimization import Optimizer"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4e5fd71c",
            "metadata": {},
            "source": [
                "### Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "id": "6a1e5559-98cb-49e8-bfc5-b10b25515740",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset shape: (391036, 4)\n"
                    ]
                }
            ],
            "source": [
                "# data_path = os.environ['DATASET_PATH']\n",
                "# datasets = np.load(data_path)\n",
                "# data_path\n",
                "\n",
                "with h5py.File(\"c:\\\\Users\\\\kaoid\\\\My Drive\\\\Estudo\\\\Poli\\\\Disciplinas\\\\PINNs\\\\acousticPINNs\\\\dataset.h5\", 'r') as f:\n",
                "  data = list(f[\"standardData\"]) # read standardized data\n",
                "  # data = list(f[\"unitData\"]) # read unit normalized data\n",
                "\n",
                "# dataset organized as list of 4 numpy.ndarray elements:\n",
                "# x coordinates, y coordinates, time steps, pressure values.\n",
                "# reorganize data in [nodeQuantity * time steps, 4] matrix with\n",
                "# one information per column\n",
                "dataMatrix = np.concatenate(([data[i][:, None] for i in range(4)]), axis = 1)\n",
                "print(f\"Dataset shape: {dataMatrix.shape}\")\n",
                "# count = dict(Counter(data[2]))\n",
                "# for k in count:\n",
                "#   print(k, count[k])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "id": "9625ab65-0688-4250-a636-05d8e57a93f7",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Input dataset shape: (1000, 30)\n",
                        "Output dataset shape: (1000, 30)\n"
                    ]
                }
            ],
            "source": [
                "# input_dataset_raw is [sensor, case]\n",
                "# input_dataset_raw = datasets['input_dataset']\n",
                "\n",
                "n_cases = 30\n",
                "n_sensors = 1000\n",
                "# choose n_sensors random points in physical domain (which has 3371 nodes)\n",
                "randNodeIDs = random.sample(range(3372), k = n_sensors)\n",
                "# [sensor, case] matrix for input_dataset. Each line contains\n",
                "# the pressure history of a specific \"sensor\" for the first n_cases time steps.\n",
                "input_dataset = np.concatenate(\n",
                "    ([dataMatrix[randNodeIDs + np.tile(3371 * step, 1), 2][:, None] for step in range(n_cases)]),\n",
                "    axis = 1\n",
                ")\n",
                "print(f\"Input dataset shape: {input_dataset.shape}\")\n",
                "\n",
                "# output_dataset_raw is [time step, ?, case]\n",
                "# output_dataset_raw = datasets['output_dataset']\n",
                "\n",
                "timeStepAmount = int(dataMatrix.shape[0] / 3371) # number of time steps\n",
                "# same as input_dataset, but for the LAST n_cases time steps\n",
                "output_dataset = np.concatenate(\n",
                "    ([dataMatrix[\n",
                "        randNodeIDs + np.tile(3371 * step, 1), 2\n",
                "    ][:, None] for step in range(timeStepAmount - n_cases, timeStepAmount)]),\n",
                "    axis = 1\n",
                ")\n",
                "print(f\"Output dataset shape: {output_dataset.shape}\")\n",
                "# time_raw = datasets['time']"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "f391346b",
            "metadata": {},
            "source": [
                "### Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "ab9ded2b-2e2b-4ce2-939a-888419b1e31d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# time_interval = [0, 120]\n",
                "# n_cases_test = 20\n",
                "# n_time_samples = 50\n",
                "latent_dim = 500\n",
                "# percentage of dataset used for testing\n",
                "testPercentage = 0.3\n",
                "# n_vars = 2 not used\n",
                "activation = \"relu\"\n",
                "trunk_layers_units = [50, 50, 50]\n",
                "branch_layers_units = [50, 50, 50]\n",
                "n_inputs = 1\n",
                "lr = 1e-3\n",
                "lambda_1 = 0.0\n",
                "lambda_2 = 1e-5\n",
                "n_epochs = 10"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "69946457",
            "metadata": {},
            "source": [
                "### Times and sensors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "id": "1e5e0aae-0477-426d-b7f0-4f747b76227b",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "timeStepAmount: 116\n",
                        "'time' vector shape: (116,)\n"
                    ]
                }
            ],
            "source": [
                "# set apart time values up to last time step\n",
                "# time_ = time_raw[time_raw <= time_interval[-1]] # datasets['time']\n",
                "# sorted 'n_time_samples' random indices of 'time_'\n",
                "# time_indices = sorted(np.random.choice(time_.shape[0], n_time_samples))\n",
                "# time = time_[time_indices]\n",
                "\n",
                "# time step values\n",
                "time = np.asarray(list(dict(Counter(dataMatrix[:, 2])).keys()))\n",
                "print(f\"timeStepAmount: {timeStepAmount}\")\n",
                "print(f\"'time' vector shape: {time.shape}\")\n",
                "# sensors_indices = np.linspace(0, time_.shape[0], n_sensors).astype(int)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "0e528663",
            "metadata": {},
            "source": [
                "### Split datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "id": "4132b6e0-1f90-4a66-bedf-a3c72c762f2a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "output_dataset_train shape: (1000, 21)\n",
                        "output_dataset_test shape: (1000, 9)\n",
                        "input_dataset_train shape: (1000, 21)\n",
                        "input_dataset_test shape: (1000, 9)\n"
                    ]
                }
            ],
            "source": [
                "# output_dataset_raw is [time step, ?, case]\n",
                "# output_dataset_train = output_dataset_raw[:, :, :n_cases]\n",
                "# output_dataset_test = output_dataset_raw[:, :, n_cases:]\n",
                "\n",
                "# output_dataset is [sensor, case]\n",
                "output_dataset_train = output_dataset[:, :round(n_cases * (1 - testPercentage))]\n",
                "output_dataset_test = output_dataset[:, round(n_cases * (1 - testPercentage)):]\n",
                "print(f\"output_dataset_train shape: {output_dataset_train.shape}\")\n",
                "print(f\"output_dataset_test shape: {output_dataset_test.shape}\")\n",
                "\n",
                "# input_dataset_raw is [sensor, case]\n",
                "# input_dataset_train = input_dataset_raw[:, :n_cases]\n",
                "# input_dataset_test = input_dataset_raw[:, n_cases:]\n",
                "\n",
                "# input_dataset is [sensor, case]\n",
                "input_dataset_train = input_dataset[:, :round(n_cases * (1 - testPercentage))]\n",
                "input_dataset_test = input_dataset[:, round(n_cases * (1 - testPercentage)):]\n",
                "print(f\"input_dataset_train shape: {input_dataset_train.shape}\")\n",
                "print(f\"input_dataset_test shape: {input_dataset_test.shape}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "4c1ef301",
            "metadata": {},
            "source": [
                "### Instants and sensors"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "7118ff5b",
            "metadata": {},
            "source": [
                "Set apart data from desired instants and sensors"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "id": "f926bccd-2c84-4ca7-9276-a1ddd9bc2bcd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(1000, 1, 21)\n",
                        "(1000, 1, 21)\n"
                    ]
                }
            ],
            "source": [
                "# output_dataset_raw is [time step, ?, case]\n",
                "# output_dataset_time_sampled = output_dataset_train[time_indices, ...]\n",
                "\n",
                "output_dataset_time_sampled = output_dataset_train[:, None, :]\n",
                "print(output_dataset_time_sampled.shape)\n",
                "\n",
                "# input_dataset_raw is [sensor, case]\n",
                "# input_dataset_sensor_sampled = input_dataset_train[sensors_indices, ...][:, None, :]\n",
                "\n",
                "input_dataset_sensor_sampled = input_dataset_train[:, None, :]\n",
                "print(input_dataset_sensor_sampled.shape)\n",
                "verify_case_index = 100"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "03998be6",
            "metadata": {},
            "source": [
                "### Inputs and outputs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "id": "1daa8c3d-0dff-4a4b-9449-28d9228b88e2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "output_target shape: (21, 1000)\n",
                        "input_branch shape: (21, 116000)\n",
                        "input_trunk shape: (3480, 1)\n"
                    ]
                }
            ],
            "source": [
                "# [time step, ?, case] -> transpose -> [case, time step, ?] -> reshape -> [case * time step, -1]\n",
                "# output_target = output_dataset_time_sampled.transpose(2, 0, 1).reshape(n_cases * n_time_samples, -1)\n",
                "\n",
                "trainSize = round(n_cases * (1 - testPercentage))\n",
                "output_target = output_dataset_time_sampled.transpose(2, 0, 1).reshape(trainSize, -1)\n",
                "\n",
                "# [sensor, None, case] -> transpose -> [case, None, sensor]\n",
                "# repeat for (1, n_time_samples, 1)\n",
                "# reshape to [n_cases * n_time_samples, -1]\n",
                "# input_branch = np.tile(input_dataset_sensor_sampled.transpose(2, 1, 0), (1, n_time_samples, 1)).reshape(n_cases * n_time_samples, -1)\n",
                "input_branch = np.tile(\n",
                "  input_dataset_sensor_sampled.transpose(2, 1, 0),\n",
                "  (1, timeStepAmount, 1)\n",
                ").reshape(trainSize, -1)\n",
                "\n",
                "# repeat time instants for (n_cases, 1)\n",
                "input_trunk = np.tile(time[:, None], (n_cases, 1))\n",
                "\n",
                "print(f\"output_target shape: {output_target.shape}\")\n",
                "print(f\"input_branch shape: {input_branch.shape}\")\n",
                "print(f\"input_trunk shape: {input_trunk.shape}\")"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "id": "0e806571",
            "metadata": {},
            "source": [
                "### Setup NNs and optimizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "id": "6da66465-9c11-429f-b2a5-dc55eb8fbaf5",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Summary of the network properties:\n",
                        "Linear operations layers:\n",
                        "\n",
                        "[ Linear(in_features=1, out_features=50, bias=True),\n",
                        "  Linear(in_features=50, out_features=50, bias=True),\n",
                        "  Linear(in_features=50, out_features=50, bias=True),\n",
                        "  Linear(in_features=50, out_features=500, bias=True)]\n",
                        "\n",
                        "\n",
                        "Activations layers:\n",
                        "\n",
                        "['relu', 'relu', 'relu', 'identity']\n",
                        "\n",
                        "\n",
                        "Initializations at each layer:\n",
                        "\n",
                        "['xavier', 'xavier', 'xavier', 'xavier']\n",
                        "Summary of the network properties:\n",
                        "Linear operations layers:\n",
                        "\n",
                        "[ Linear(in_features=1000, out_features=50, bias=True),\n",
                        "  Linear(in_features=50, out_features=50, bias=True),\n",
                        "  Linear(in_features=50, out_features=50, bias=True),\n",
                        "  Linear(in_features=50, out_features=500, bias=True)]\n",
                        "\n",
                        "\n",
                        "Activations layers:\n",
                        "\n",
                        "['relu', 'relu', 'relu', 'identity']\n",
                        "\n",
                        "\n",
                        "Initializations at each layer:\n",
                        "\n",
                        "['xavier', 'xavier', 'xavier', 'xavier']\n"
                    ]
                }
            ],
            "source": [
                "# Configuration for the fully-connected network\n",
                "config_trunk = {\n",
                "  'layers_units': trunk_layers_units,  # Hidden layers\n",
                "  'activations': activation,\n",
                "  'input_size': n_inputs,\n",
                "  'output_size': latent_dim,\n",
                "  'name': 'trunk_net'\n",
                "}\n",
                "\n",
                " # Configuration for the fully-connected network\n",
                "config_branch = {\n",
                "  'layers_units': branch_layers_units,  # Hidden layers\n",
                "  'activations': activation,\n",
                "  'input_size': n_sensors,\n",
                "  'output_size': latent_dim,\n",
                "  'name': 'branch_net'\n",
                "}\n",
                "\n",
                "# Instantiating and training the surrogate model\n",
                "trunk_net = DenseNetwork(**config_trunk)\n",
                "\n",
                "# Instantiating and training the surrogate model\n",
                "branch_net = DenseNetwork(**config_branch)\n",
                "\n",
                "trunk_net.summary()\n",
                "branch_net.summary()\n",
                "\n",
                "optimizer_config = {'lr': lr}\n",
                "\n",
                "# Maximum derivative magnitudes to be used as loss weights\n",
                "maximum_values = (1/np.linalg.norm(output_target, 2, axis=0)).tolist()\n",
                "\n",
                "params = {'lambda_1': lambda_1, 'lambda_2': lambda_2, 'weights': maximum_values}\n",
                "\n",
                "input_data = {'input_branch': input_branch, 'input_trunk': input_trunk}\n",
                "\n",
                "# The DeepONet receives the two instances in order to construct \n",
                "# the trunk and the branch components\n",
                "op_net = DeepONet(trunk_network=trunk_net, branch_network=branch_net, var_dim=2, model_id=\"LotkaVolterra\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cb9cb1e3-d92b-40b5-a47b-c647d4a6f0ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "optimizer = Optimizer('adam', params=optimizer_config)\n",
                "optimizer.fit(op_net, input_data=input_data, target_data=output_target,\n",
                "                      n_epochs=n_epochs, loss=\"wrmse\", params=params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "1b656176-4c34-4b05-9282-40c6da6135c3",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'n_cases_test' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m n_tests_choices \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[1;32m----> 2\u001b[0m test_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(n_cases_test, n_tests_choices)\n\u001b[0;32m      3\u001b[0m time_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinspace(\u001b[39m0\u001b[39m, time_interval[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m2000\u001b[39m)[:,\u001b[39mNone\u001b[39;00m]\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m test_indices[::\u001b[39m10\u001b[39m]:\n",
                        "\u001b[1;31mNameError\u001b[0m: name 'n_cases_test' is not defined"
                    ]
                }
            ],
            "source": [
                "n_tests_choices = 100\n",
                "test_indices = np.random.choice(n_cases_test, n_tests_choices)\n",
                "time_test = np.linspace(0, time_interval[-1], 2000)[:,None]\n",
                "\n",
                "for index in test_indices[::10]:\n",
                "    \n",
                "    target_test = output_dataset_test[:, :, index]\n",
                "    input_test_ = input_dataset_test[None, sensors_indices, index]\n",
                "    input_test = np.tile(input_test_, (2000, 1))\n",
                "    evaluation = op_net.eval(trunk_data=time_test, branch_data=input_test)\n",
                "    \n",
                "    plt.plot(time_raw, target_test[:,0], label=\"Exact\")\n",
                "    plt.plot(time_test, evaluation[:,0], label=\"Approximated\")\n",
                "    plt.legend()\n",
                "    plt.xlim(0, 120)\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
